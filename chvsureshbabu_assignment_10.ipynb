{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vSureshbabuchitturi/Data-Science-NLP-/blob/main/chvsureshbabu_assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9691c16c",
      "metadata": {
        "id": "9691c16c"
      },
      "source": [
        "# Assignment 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f07eee",
      "metadata": {
        "id": "92f07eee"
      },
      "source": [
        "# Please save your file with your name.ipynb and share this Jupter notebook with solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234ec53d",
      "metadata": {
        "id": "234ec53d"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "# What is stmeming and lemmatiation? Why it is used?\n",
        "Stemming and lemmatization are techniques used to reduce words to their base or root form.\n",
        "Stemming removes prefixes or suffixes from words to return a root word (e.g., \"running\" → \"run\"),\n",
        "though it may result in non-meaningful stems (e.g., \"better\" → \"bet\").\n",
        "\n",
        "Lemmatization, on the other hand, uses vocabulary and morphological analysis to return\n",
        "the proper base form of a word (e.g., \"running\" → \"run\" and \"better\" → \"good\").\n",
        "\n",
        "These techniques are used in natural language processing (NLP) to normalize words,\n",
        "reduce variability in word forms, and improve the accuracy of text analysis by\n",
        "ensuring that variations of a word are treated as a single token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3p3J6pHNv0cl"
      },
      "id": "3p3J6pHNv0cl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8222ac9",
      "metadata": {
        "id": "e8222ac9"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "# What are stopwords? Give few examples.\n",
        "Stopwords are common words that are filtered out during text processing because they\n",
        "carry little meaningful information in text analysis.\n",
        "These words, such as \"the,\" \"is,\" \"in,\" \"on,\" \"and,\" are used frequently in the language\n",
        "but dont provide significant value when analyzing text.\n",
        "Removing stopwords helps reduce dimensionality and computational complexity in tasks like\n",
        "text classification, topic modeling, and information retrieval. For example, in the sentence\n",
        "\"The quick brown fox jumps over the lazy dog,\" the words \"the\" and \"over\" would be considered\n",
        "stopwords and typically removed in preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5622a4",
      "metadata": {
        "id": "5a5622a4"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "# What are the disadvantages of TFIDF feature extraction?\n",
        "While TF-IDF (Term Frequency-Inverse Document Frequency) is useful for transforming text\n",
        "into numerical features, it has several drawbacks.\n",
        "First, it creates a high-dimensional and sparse matrix, which can be computationally\n",
        "expensive for large corpora.\n",
        "\n",
        "Second, TF-IDF does not capture semantic relationships between words, meaning it cant\n",
        "recognize that \"dog\" and \"canine\" are related. It also relies heavily on frequency and\n",
        "does not account for word order or context, limiting its effectiveness in tasks requiring\n",
        "deeper semantic understanding.\n",
        "Additionally, rare words with high document frequency may not be handled well by TF-IDF,\n",
        "leading to poor representations in some cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9e0a71",
      "metadata": {
        "id": "2e9e0a71"
      },
      "outputs": [],
      "source": [
        "# Question 4\n",
        "# Why TFIDF is better than Bag of words feature extraction?\n",
        "\n",
        "TF-IDF improves upon the Bag of Words (BoW) model by not only counting the frequency of words\n",
        "but also considering their importance in the context of the entire corpus.\n",
        "While BoW treats all words equally, TF-IDF adjusts word frequencies by the inverse document\n",
        "frequency, down-weighting common words that appear in many documents and emphasizing terms\n",
        "that are rare but relevant to a specific document.\n",
        "This makes TF-IDF more suitable for information retrieval, classification, and clustering tasks,\n",
        "where distinguishing between important and unimportant words is essential.\n",
        "It also provides better results in cases where word frequency alone doesn’t capture the true\n",
        "relevance of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0c7824",
      "metadata": {
        "id": "0e0c7824"
      },
      "outputs": [],
      "source": [
        "# Question 5\n",
        "# What is named entity extraction?\n",
        "Named Entity Extraction (NER) is a process in natural language processing (NLP)\n",
        "that identifies and classifies proper nouns or entities in text, such as persons,\n",
        "organizations, locations, and dates. For example, in the sentence\n",
        "\"Barack Obama visited the White House on Monday,\" the NER system would extract\n",
        "\"Barack Obama\" as a person, \"White House\" as an organization, and \"Monday\" as a date.\n",
        "NER helps in structuring unstructured text and is used in various applications like\n",
        "information retrieval, question answering, and data extraction.\n",
        "Advanced NER systems use machine learning or deep learning models to improve accuracy and\n",
        "handle ambiguities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fd1589",
      "metadata": {
        "id": "a8fd1589"
      },
      "outputs": [],
      "source": [
        "# Question 6\n",
        "# What is the difference between CBOW and skip gram architecture?\n",
        "CBOW (Continuous Bag of Words) and Skip-Gram are two architectures used in Word2Vec\n",
        "for generating word embeddings. CBOW predicts the center word given a set of context words,\n",
        "making it faster and more computationally efficient, especially when working with large atasets.\n",
        "\n",
        "On the other hand, Skip-Gram predicts the context words surrounding a given center word.\n",
        "Skip-Gram is generally more effective for capturing detailed semantic relationships\n",
        "and handling rare words, but it is slower as it requires multiple predictions per word.\n",
        "Both methods are designed to capture the context of words and learn their vector representations\n",
        "based on co-occurrence in the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4475e887",
      "metadata": {
        "id": "4475e887"
      },
      "outputs": [],
      "source": [
        "# Questions 7\n",
        "# Why word2vec is better than TFIDF features?\n",
        "Word2Vec is superior to TF-IDF because it captures the semantic meaning and context of words.\n",
        "While TF-IDF only considers word frequency and document occurrence, Word2Vec creates continuous\n",
        "vector representations that reflect word meanings and their relationships to other words.\n",
        "For example, it can understand that words like “king” and “queen” are semantically similar\n",
        "based on the context in which they appear, whereas TF-IDF does not account for these\n",
        "relationships.\n",
        "Word2Vec is also more efficient for use in deep learning models,\n",
        "as it provides dense, low-dimensional vectors compared to the sparse, high-dimensional matrices\n",
        "produced by TF-IDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2577759d",
      "metadata": {
        "id": "2577759d"
      },
      "outputs": [],
      "source": [
        "# Question 8\n",
        "# What is the main issue faced by word2vec features? How it is addressed by FastText?\n",
        "\n",
        "The main issue with Word2Vec is its inability to handle out-of-vocabulary (OOV) words and words\n",
        "with rare or complex morphology. Word2Vec represents words as discrete tokens,\n",
        "and for words not seen during training, it cannot provide a meaningful embedding.\n",
        "\n",
        "FastText addresses this by representing words as subword units (n-grams of characters),\n",
        "which allows the model to generate embeddings for OOV words by leveraging the subwords that\n",
        "appear in other words. For example, even if \"unhappiness\" wasn’t in the training corpus,\n",
        "FastText can still generate a useful vector based on the subwords \"un,\" \"hap,\" and \"ness,\"\n",
        "improving the model's ability to generalize across various languages and word forms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77b9d9e",
      "metadata": {
        "id": "a77b9d9e"
      },
      "outputs": [],
      "source": [
        "# Question 9\n",
        "# Write steps of building similar document recommendation project based on the document shared by user.\n",
        "\n",
        "Problem Understanding and Data Collection:\n",
        "\n",
        "Objective: Identify and recommend documents that are similar to a given document.\n",
        "Input: A corpus of documents (e.g., text files, articles, product descriptions).\n",
        "Output: A ranked list of documents similar to the user's input document.\n",
        "\n",
        "Data Preprocessing:\n",
        "Text Cleaning:Remove unnecessary characters, punctuation, special symbols, and HTML tags.\n",
        "Lowercasing:Convert all text to lowercase for uniformity.\n",
        "Stopword Removal:Remove common stopwords (e.g., the, is, and) to reduce noise.\n",
        "Tokenization:Split text into individual words or tokens.\n",
        "Stemming/Lemmatization:Reduce words to their base or root form (e.g., running → run).\n",
        "\n",
        "Feature Extraction:Convert text data into numerical format that machine learning models can process.\n",
        "Options for Feature Extraction:\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency): Create a sparse matrix representing the importance of terms in each document.\n",
        "Word Embeddings: Use Word2Vec, FastText, or pre-trained models like GloVe to generate dense word vectors.\n",
        "Document Embeddings:Use Doc2Vec or pre-trained models like Sentence-BERT for capturing the meaning of an entire document.\n",
        "\n",
        "\n",
        "Similarity Computation:\n",
        "Measure Similarity:Use cosine similarity, Euclidean distance, or Jaccard similarity to compute the closeness between document vectors.\n",
        "For embeddings, cosine similarity is typically used as it works well with high-dimensional vectors.\n",
        "\n",
        "Similarity Matrix:Compute pairwise similarity scores for all documents in the corpus and store them in a matrix.\n",
        "Building the Recommendation System:\n",
        "For a given document:Extract its feature vector.\n",
        "Compare it against all other documents in the corpus using the similarity metric.\n",
        "Rank documents based on their similarity scores.\n",
        "Select the top N most similar documents as recommendations.\n",
        "\n",
        "User Interface: Create an interface where users can input a document or query.\n",
        "Display recommended documents with their similarity scores or excerpts for better user experience.\n",
        "Evaluation: Use metrics to evaluate the effectiveness of the recommendation system:\n",
        "Precision@K: Measures the relevance of the top K recommendations.\n",
        "Recall: Measures how many relevant documents are recommended.\n",
        "\n",
        "Mean Reciprocal Rank (MRR): Evaluates the ranking quality of recommendations.\n",
        "\n",
        "Optimization: Fine-tune preprocessing steps and feature extraction methods.\n",
        "Experiment with different similarity metrics or embeddings.\n",
        "\n",
        "Use topic modeling (e.g., Latent Dirichlet Allocation) to add another layer of semantic similarity.\n",
        "\n",
        "Deployment: Package the recommendation system into an API or web application.\n",
        "\n",
        "Use tools like Flask, FastAPI, or Django for serving recommendations.\n",
        "\n",
        "Integrate with front-end applications for seamless user interaction.\n",
        "Example Workflow:\n",
        "Input: User provides a document about \"machine learning applications.\"\n",
        "\n",
        "Process:Preprocess the input and convert it into a feature vector.\n",
        "Compare the vector with all documents in the corpus using cosine similarity.\n",
        "Rank the documents based on similarity scores.\n",
        "\n",
        "Output:A ranked list of documents related to \"machine learning applications.\"\n",
        "\n",
        "Advanced Features:\n",
        "Dynamic Corpus Updates:\n",
        "Handle real-time additions to the document corpus without retraining.\n",
        "Context-Aware Recommendations:\n",
        "Use transformers like BERT or RoBERTa for deeper contextual understanding.\n",
        "Interactive Feedback:\n",
        "Allow users to refine recommendations by marking documents as relevant or irrelevant.\n",
        "By following these steps, you can build a robust similar document recommendation system that efficiently identifies\n",
        "and recommends relevant content based on user input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08b6885",
      "metadata": {
        "id": "d08b6885"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a54af6",
      "metadata": {
        "id": "39a54af6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9622f35",
      "metadata": {
        "id": "f9622f35"
      },
      "outputs": [],
      "source": [
        "# Question 10\n",
        "# Build a classification model using word2vec features for classifying the reviews of movie.\n",
        "# Data provided: 'imdb_reviews.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "994e4026",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "994e4026",
        "outputId": "6265297c-70a4-48ee-c7cc-37bf3896bcd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.838\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84      4499\n",
            "           1       0.83      0.85      0.84      4501\n",
            "\n",
            "    accuracy                           0.84      9000\n",
            "   macro avg       0.84      0.84      0.84      9000\n",
            "weighted avg       0.84      0.84      0.84      9000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # This line was added to download the missing data package\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path ='/content/sample_data/imdb_reviews.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "# Display the first few rows of the dataset\n",
        "data.head(), data.info()\n",
        "\n",
        "# Step 1: Preprocess the reviews\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "data['processed_review'] = data['review'].apply(preprocess_text)\n",
        "\n",
        "# Step 2: Train a Word2Vec model on the processed reviews\n",
        "reviews_list = data['processed_review'].tolist()\n",
        "word2vec_model = Word2Vec(sentences=reviews_list, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 3: Generate feature vectors for each review\n",
        "def review_to_vector(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "data['vector'] = data['processed_review'].apply(lambda x: review_to_vector(x, word2vec_model))\n",
        "\n",
        "# Convert feature vectors to a NumPy array\n",
        "X = np.array(data['vector'].tolist())\n",
        "y = (data['sentiment'] == 'positive').astype(int)  # Convert to binary labels\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train a classification model (Random Forest)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183a44b6",
      "metadata": {
        "id": "183a44b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf4ee45",
      "metadata": {
        "id": "aaf4ee45"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}